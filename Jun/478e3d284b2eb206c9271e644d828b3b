TikTok, the short-video social-media platform popular among teens, said it removed more than 7 million accounts belonging to children under 13 during the first quarter, taking a significant step in enforcing the required age of users on the app. Of more than 11.1 million accounts that were removed for violating the app’s guidelines, 7.26 million of them were from users suspected of being under the age guidelines, the company said in a blog post on Wednesday. It was the first time TikTok published the number of underage accounts it expunged. In the U.S., internet sites are required to obtain parental permission before collecting data on children under 13, according to the Children’s Online Privacy Protection Act. But many kids fudge their age and create accounts anyway across social media sites from Instagram to YouTube. TikTok, which is owned by China’s ByteDance Ltd., has drawn particular scrutiny because of the vast amount of data its sophisticated algorithm collects and its popularity especially among young people. Last year, the New York Times said TikTok classified more than a third of its daily users in the U.S. as being 14 or younger. In 2019, TikTok was forced to pay the U.S. Federal Trade Commission a record $5.7 million fine for unlawfully collecting children’s data, including names, email address and locations of children who used the app. Since then, TikTok has adjusted several features to make the platform safer for its fans. About two years ago it launched a dedicated section on the app for children 12 and under called TikTok for Younger Users. The space offers “a curated viewing experience with additional safeguards and privacy protections” according to the company. The walled-off portion of the app doesn’t permit sharing personal information, puts limitations on displayed content and also doesn’t allow its users to post videos or comments. Other age-accommodative changes include a new default privacy setting for accounts age 13-15, launched in January, that requires teenage users to approve followers to view their videos. Last August the company also introduced a new Family Pairing setting to allow parents to monitor and control their children’s screen time on the app. Along with removing accounts, TikTok also constantly monitors published content. The report Wednesday revealed that in the first three months of the year, 8.54 million videos were removed in the U.S. alone. Of those taken down, 36.8% were for “minor safety” reasons, 21.1% for involving illegal activities and 15.6% for violating policies on adult nudity and sexual activities. TikTok said starting with this report, it will begin publishing its insights related to the enforcement of community guidelines on a quarterly basis.