Facebook Inc. will begin doling out harsher punishments for individual accounts that repeatedly share false or misleading posts, an expansion of the social network’s efforts to crack down on misinformation. Under the new system, Facebook will “reduce the distribution of all posts” from people who routinely share misinformation, making it harder for their content to be seen by others on the service. The company already does this for Pages and Groups that post misinformation, but it hadn’t previously extended the same policy to individual users. Facebook does limit the reach of posts that have been flagged by fact-checkers, but there wasn’t a broader penalty for account holders who share misinformation. Facebook declined to specify how many times a user’s posts have to be flagged before the new punishment kicks in. The Menlo Park, California-based company will also start showing users a pop-up message if they click to “like” a page that routinely shares misinformation, alerting them that fact-checkers have previously flagged that page’s posts. “This will help people make an informed decision about whether they want to follow the Page,” the company wrote in a blog post. The moves are the latest in Facebook’s effort to curb the spread of misinformation on its networks— an ongoing challenge, especially concerning elections and Covid-19. Facebook created dedicated information hubs for topics like Covid-19 and climate change as a way to present users with reliable information, but has struggled overall to keep up with rumors and misleading posts from its nearly 3 billion users.